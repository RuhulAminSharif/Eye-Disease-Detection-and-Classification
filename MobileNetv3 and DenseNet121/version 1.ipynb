{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:17:36.825923Z","iopub.execute_input":"2025-03-07T18:17:36.826396Z","iopub.status.idle":"2025-03-07T18:17:48.092328Z","shell.execute_reply.started":"2025-03-07T18:17:36.826363Z","shell.execute_reply":"2025-03-07T18:17:48.091220Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:30:37.380955Z","iopub.execute_input":"2025-03-07T18:30:37.381225Z","iopub.status.idle":"2025-03-07T18:30:50.940206Z","shell.execute_reply.started":"2025-03-07T18:30:37.381202Z","shell.execute_reply":"2025-03-07T18:30:50.939320Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\n\n# Define the directory path to your dataset\ndata_dir = '/kaggle/input/eye-disease-image-dataset/Augmented Dataset/Augmented Dataset/'\n\n# Create an ImageDataGenerator with rescaling and a validation split.\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2  # Use 20% of the data for validation\n)\n\n# Training data generator\ntrain_generator = datagen.flow_from_directory(\n    data_dir,\n    target_size=(224, 224),  # VGG16 expects 224x224 images\n    batch_size=32,\n    class_mode='categorical',\n    subset='training'\n)\n\n# Validation data generator\nvalidation_generator = datagen.flow_from_directory(\n    data_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical',\n    subset='validation'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:30:53.518593Z","iopub.execute_input":"2025-03-07T18:30:53.519151Z","iopub.status.idle":"2025-03-07T18:31:11.746163Z","shell.execute_reply.started":"2025-03-07T18:30:53.519121Z","shell.execute_reply":"2025-03-07T18:31:11.745253Z"}},"outputs":[{"name":"stdout","text":"Found 12997 images belonging to 10 classes.\nFound 3245 images belonging to 10 classes.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:31:15.838158Z","iopub.execute_input":"2025-03-07T18:31:15.838427Z","iopub.status.idle":"2025-03-07T18:31:21.591952Z","shell.execute_reply.started":"2025-03-07T18:31:15.838405Z","shell.execute_reply":"2025-03-07T18:31:21.591257Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n\nfor layer in base_model.layers:\n    layer.trainable = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:31:30.701458Z","iopub.execute_input":"2025-03-07T18:31:30.701813Z","iopub.status.idle":"2025-03-07T18:31:30.705657Z","shell.execute_reply.started":"2025-03-07T18:31:30.701783Z","shell.execute_reply":"2025-03-07T18:31:30.704639Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\nx = base_model.output\nx = Flatten()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)  \npredictions = Dense(train_generator.num_classes, activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:31:32.933659Z","iopub.execute_input":"2025-03-07T18:31:32.934012Z","iopub.status.idle":"2025-03-07T18:31:32.967494Z","shell.execute_reply.started":"2025-03-07T18:31:32.933983Z","shell.execute_reply":"2025-03-07T18:31:32.966593Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:31:38.453159Z","iopub.execute_input":"2025-03-07T18:31:38.453465Z","iopub.status.idle":"2025-03-07T18:31:38.461551Z","shell.execute_reply.started":"2025-03-07T18:31:38.453440Z","shell.execute_reply":"2025-03-07T18:31:38.460624Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // validation_generator.batch_size,\n    epochs=10  # Feel free to adjust the number of epochs\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:31:40.814416Z","iopub.execute_input":"2025-03-07T18:31:40.814742Z","iopub.status.idle":"2025-03-07T18:50:00.093915Z","shell.execute_reply.started":"2025-03-07T18:31:40.814695Z","shell.execute_reply":"2025-03-07T18:50:00.093031Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 651ms/step - accuracy: 0.3345 - loss: 1.8929 - val_accuracy: 0.4864 - val_loss: 1.5223\nEpoch 2/10\n\u001b[1m  1/406\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 162ms/step - accuracy: 0.5000 - loss: 1.5129","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.5000 - loss: 1.5129 - val_accuracy: 0.3846 - val_loss: 1.6211\nEpoch 3/10\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 493ms/step - accuracy: 0.4881 - loss: 1.4433 - val_accuracy: 0.5486 - val_loss: 1.3514\nEpoch 4/10\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220us/step - accuracy: 0.3438 - loss: 1.5076 - val_accuracy: 0.3846 - val_loss: 1.5751\nEpoch 5/10\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 481ms/step - accuracy: 0.5565 - loss: 1.2617 - val_accuracy: 0.5387 - val_loss: 1.2795\nEpoch 6/10\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218us/step - accuracy: 0.4688 - loss: 1.3906 - val_accuracy: 0.5385 - val_loss: 1.4809\nEpoch 7/10\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 486ms/step - accuracy: 0.5760 - loss: 1.2051 - val_accuracy: 0.5736 - val_loss: 1.2447\nEpoch 8/10\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207us/step - accuracy: 0.4062 - loss: 1.6052 - val_accuracy: 0.5385 - val_loss: 0.9978\nEpoch 9/10\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 489ms/step - accuracy: 0.6048 - loss: 1.1241 - val_accuracy: 0.6067 - val_loss: 1.1513\nEpoch 10/10\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207us/step - accuracy: 0.6875 - loss: 0.7545 - val_accuracy: 0.3077 - val_loss: 1.9344\n","output_type":"stream"}],"execution_count":10}]}